---
title: "Projet Data Mining"
subtitle: "**Etude de la base de données *Onlinenewspopularity* **"
author: "Léa BRASSEUR & Benjamin CHAUVET"
date: ' '
lang: "fr"
fontsize: 11pt
geometry: a4paper,top=2cm,bottom=2cm,left=1.5cm,right=1.5cm
header-includes: 
- \usepackage{float} 
- \floatplacement{figure}{H} 
- \floatplacement{table}{H} 
output:
  pdf_document: 
    toc: true
    number_section: true 
    keep_tex: true
  html_document: 
    toc: true
    toc_float: true
    number_section: false
    highlight: "espresso"
    theme: flatly
    df_print: paged
    dev: png
editor_options: 
  chunk_output_type: inline
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align="center", fig.height = 4, fig.width = 6)
```

```{r, echo = FALSE}
library(FactoMineR)
library(factoextra)
library(kableExtra)
library(DT)
library(gridExtra)
library(tidyr)
library(corrplot)
library(gplots)
library(ggcorrplot)
library(ggplot2)
library(GGally)
library(stringr)
library(stargazer)
library(ggpubr)
library(tidyverse)
library(tidymodels)
library(discrim)
library(kknn)
library(ROCR)
library(class)
library(MASS)
library(rpart)
library(randomForest)
library(ada)
library(rpart.plot)
library(vip)
library(caret)
library(pROC)
library(randomForest)
library(e1071)
library(grDevices)
```

```{r, echo = FALSE}
df <- read.table(file="OnlineNewsPopularity.csv", sep=",",dec=".",header=TRUE)
```

\newpage 

# Introduction 

## Objectifs 

Notre base de données est composée de **`r ncol(df)`** variables sur **`r nrow(df)`** articles publiés sur le site [Mashable](https://mashable.com/).
Au premier semestre, nous avions étudié ce qui influençait le nombre de partages des articles. 
Nous avions conclu que certains thèmes d'articles sont plus partagés et que les articles publiés le weekend sont plus populaires. Tout comme certaines caractéristiques sur le contenu des articles impactent le nombre de partages.

Dans cette étude, nous allons déterminer le meilleur modèle prédictif de notre base de données. L'objectif étant de prédire selon les caractéristiques d'un article s'il sera populaire ou non.

## Nettoyage et recodage de la base de données 

Tout d'abord, nous avons créé un autre *dataframe* basé sur le premier, avec les variables **url** et **timedelta** en moins car elles étaient non prédictives. Nous avons également supprimé les articles avec 0 mots car nous ne trouvions pas cela pertinent dans notre analyse.

```{r,echo = FALSE}
dfbis <- df[,-c(1,2)] 
```

```{r, echo=FALSE}
dfbis <- dfbis[dfbis$n_tokens_content>0,]
```

Puis, nous avons transformé nos variables quantitatives binaires sur les thèmes des articles et les jours de publication de la semaine en qualitatives.

```{r, echo=FALSE}
dfbis <- dfbis %>%  mutate(channel = case_when(
                 data_channel_is_lifestyle==1 ~ "Lifestyle",
                 data_channel_is_entertainment==1 ~ "Entertainment",
                 data_channel_is_bus==1 ~ "Business",
                 data_channel_is_socmed==1 ~ "Social Media",
                 data_channel_is_tech==1 ~ "Technologie",
                 data_channel_is_world==1 ~ "World", 
                 data_channel_is_lifestyle==0 & data_channel_is_entertainment==0 & data_channel_is_bus==0 &                      data_channel_is_socmed==0 & data_channel_is_tech==0 & data_channel_is_world==0 ~ "Autre"))
dfbis <- dfbis[,-c(12:17)]
```

```{r, echo=FALSE}
dfbis <- dfbis %>% mutate(weekday = case_when(
                                  weekday_is_monday==1 ~ "Lundi",
                                  weekday_is_tuesday==1 ~ "Mardi",
                                  weekday_is_wednesday==1  ~ "Mercredi",
                                  weekday_is_thursday==1 ~ "Jeudi",
                                  weekday_is_friday==1 ~ "Vendredi",
                                  weekday_is_saturday==1 ~ "Samedi",
                                  weekday_is_sunday==1 ~ "Dimanche"))

dfbis <- dfbis[,-c(24:30)]
```

```{r, echo=FALSE}
dfbis <- dfbis %>% mutate(is_weekend = case_when(
                 is_weekend==1 ~ "Weekend",
                 is_weekend==0 ~ "Semaine"))
```

```{r, echo=FALSE}
dfbis <- dfbis %>% mutate(popularity = case_when(
                shares<=1400 ~ "Unpopular",
                shares>1400 ~ "Popular"))
```

Par la suite, nous avons crée notre variable à prédire **Popularity** à partir du nombre de partages.

Le but de nos analyses étant donc de prédire l'appartenance des différents articles à la classe *Popular* ou *Unpopular* en fonction des caractéristiques de ceux-ci. Nous avons choisi cette séparation de la variable de popularité autour de la médiane pour obtenir deux classes avec des effectifs homogènes.

```{r, echo=FALSE}
table(dfbis$popularity) %>% kable(caption = "Répartition de la variable popularity", col.names = c("Popularity", "Effectif")) %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover"))
```


## Echantillonage 

```{r, echo = FALSE}
set.seed(1)
```

Nous avons constaté que la variable **n_non_stop_words** est constante à l'intérieur des différents groupes, nous sommes donc obligés de la supprimer pour que les modèles suivants fonctionnent. De plus, nous avons supprimé la variable **shares** sur le nombre de partages car  celle nous a permis de créer la variable de popularité.

```{r, echo=FALSE}
dfbis <- dfbis[,-c(4,46)]
```

```{r, echo=FALSE, include=FALSE}
dfbis_split <- initial_split(dfbis,strata=popularity, prop=2/3)
dfbis_split
dfbis_train <- training(dfbis_split)
dfbis_test <- testing(dfbis_split)
```

Nous séparons ensuite notre base de données en deux échantillons distincts : l'un d'apprentissage et l'autre de test. Nous avons choisi une proportion de 2/3 pour notre échantillon d'apprentissage car notre de base de données étant assez fournie, nous avions grâce à ce découpage des échantillons d'apprentissage et de test assez importants pour pouvoir créer et tester nos modèles.

```{r}
data.frame(nrow(dfbis_train), nrow(dfbis_test)) %>% kable(caption = "Répartition des échantillons", col.names = c("Apprentissage", "Test")) %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover"))
```

<br>

\newpage

# Analyses factorielles discriminantes 
## Linear discriminant analysis

Nous avons créé notre modèle dans le but de prédire l'appartenance des articles aux classes de popularité grâce aux **`r ncol(dfbis)-1`** variables présentes dans notre base de données. Puis, par la suite nous avons appliqué notre échantillon test à ce modèle.

```{r, echo=FALSE}
load("lda_model.RData")
res_lda <- predict(lda_model, dfbis_test)
```

Pour mesurer la qualité du modèle nous avons créé une matrice de confusion. Et cette matrice nous a ensuite permis de calculer les différentes erreurs ayant été faites au cours de la prédiction. Par la suite, nous avons pu faire les courbes ROC associées à ces erreurs.

```{r, echo=FALSE}
realite <- dfbis_test$popularity
prediction <- res_lda$class
tab <- table(prediction, realite)
addmargins(tab) %>% kable(caption = "Matrice de confusion") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2, ""))
```

Puis, nous avons fait la division des prédictions par la somme des valeurs de la réalité.

```{r, echo=FALSE}
round(prop.table(table(prediction, realite), margin = 2), 3) %>%
kable(caption = "Proportion en fonction de la réalité") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2))
```

```{r, echo=FALSE}
ERR_lda <- round((tab[1,2] + tab[2,1]) / sum(tab),3)
```

- L'erreur globale est de `r ERR_lda`. C'est-à-dire que `r ERR_lda*100`% de la base de données est mal estimé.

```{r, echo=FALSE}
ERR1_lda <- round(1-(tab[2,2]/sum(tab[,2])),3)
```

- L'erreur de type I est de `r ERR1_lda` et correspond à 1 - la spécificité. Il s'agit du taux de faux positifs, c'est-à-dire la part d'articles prédits populaires alors qu'ils sont en réalité impopulaires.
La spécificité étant la capacité à identifier les "vrais" articles impopulaires. 

```{r, echo=FALSE}
ERR2_lda <- round(1-(tab[1,1]/sum(tab[,1])),3)
```

- L'erreur de type II est de `r ERR2_lda` et correspond à 1 - la sensibilité. Il s'agit du taux de faux negatifs, c'est-à-dire la part d'articles prédits impopulaires alors qu'ils sont en réalité populaires.
La sensibilité étant la capacité à identifier les "vrais" articles populaires.


```{r, echo=FALSE, fig.height = 3.2, fig.width = 5.2}
popular<-sum(dfbis_test$popularity=="Popular") 
unpopular<-sum(dfbis_test$popularity=="Unpopular")
Error<-NULL 
ErrorI<-NULL
ErrorII<-NULL 
for(i in 1:101){
  c<-(i-1)/200 
  Prediction <- rep("Popular", nrow(dfbis_train)) 
  Prediction[res_lda$posterior[,2]>c]<-"Unpopular"
  Error[i]<-sum(Prediction!=realite)/nrow(dfbis_train) 
  ErrorI[i]<-sum((Prediction=="Popular")&(realite=="Unpopular"))/unpopular
  ErrorII[i]<-sum((Prediction=="Unpopular")&(realite=="Popular"))/popular
  } 

seuil <- seq(0, 0.5, by = (0.5/100))
df_error <- cbind(seuil, Error, ErrorI, ErrorII)
df_error <- as.data.frame(df_error)

p <- ggplot(data=df_error)  + geom_line(data=df_error, aes(x=seuil, y=ErrorI, colour = "1-Spécificité (Erreur type I)"))+ geom_line(data=df_error, aes(x=seuil, y=ErrorII, colour = "1-Sensibilité (Erreur type II)")) + geom_line(data=df_error, aes(x=seuil, y=Error, colour = "Erreur Globale")) + ggtitle("Graphiques des différentes erreurs de la LDA")+theme_minimal()+ labs(color = "Erreurs")+ scale_color_manual(values=c("steelblue", "violetred", "slategray"))
p
```
Grâce à ce graphique nous pouvons voir que l'erreur de type II est supérieure aux autres, ce qui signifie que notre modèle a plus de mal à prédire les articles étant populaires.


```{r, echo=FALSE}
acc <- round(tab[1,1]+tab[2,2])/sum(tab)
```

La matrice de confusion nous permet également de calculer l'accuracy qui est de : `r round(acc,3)`. Ce qui veut dire que
`r round(acc,3)*100`% de la base de données est bien prédite.

```{r, echo=FALSE, include=FALSE}
sol <- confusionMatrix(as.factor(realite), prediction)
sol$byClass %>% kable(caption = "Sommaire") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover"))
```

Nous pouvons par la suite créer une courbe ROC grâce à la spécificité et à la sensibilité.

```{r, echo=FALSE, fig.height = 3.2, fig.width = 5.2}
roc_lda <- roc(realite, res_lda$posterior[, 2])
ggroc(roc_lda) + ggtitle("Courbe ROC LDA") +
  geom_abline(intercept = 1, slope = 1, lty = "dashed") + theme_minimal()
```

```{r, echo=FALSE}
auc_lda <- round(roc_lda$auc[1],3)
```

Ici notre AUC, qui représente l'aire sous la courbe est de `r auc_lda`. Elle est plus élevée que 0.5 ce qui veut dire que nous arrivons à collecter une information, mais elle est relativement inférieure à 1, les prédictions ne sont donc pas optimales.

## Quadratic  discriminant analysis

Nous avons créé une deuxième base de données, toujours en partant de la base de données initiale. Dans celle-ci nous avons été obligés de supprimer la variable constante **n_non_stop_words**. Et également d'enlever les variables **weekday_is_friday**, **is_weekend** et **LDA_04** pour cause de colinéarité. 

```{r, echo=FALSE}
dfbis2 <- df[,-c(1,2,6,36,39,44)] 
dfbis2 <- dfbis2[dfbis2$n_tokens_content>0,]

dfbis2 <- dfbis2 %>% mutate(popularity = case_when(
                shares<=1400 ~ "Unpopular",
                shares>1400 ~ "Popular"))
dfbis2 <- dfbis2[,-55]
dfbis2[,55] <- as.factor(dfbis2[,55])
```

Nous séparons ensuite notre base de données en deux échantillons, avec toujours une proportion de 2/3 pour celui d'apprentissage. Puis nous effectuons notre QDA.

```{r, echo=FALSE, include=FALSE}
set.seed(1)
dfbis2_split <- initial_split(dfbis2, strata=popularity, prop=2/3)
dfbis2_split
dfbis2_train <- training(dfbis2_split)
dfbis2_test <- testing(dfbis2_split)
```

```{r, echo=FALSE}
load("qda_mod.RData")
qda_res <- predict(qda_mod, dfbis2_test)
```

Le modèle nous donne la matrice de confusion ci-dessous.

```{r, echo=FALSE}
prediction <- qda_res$class
realite <- dfbis2_test$popularity
tab2 <- table(prediction, realite)
addmargins(tab2) %>% kable(caption = "Matrice de confusion") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2, ""))
```
Puis le tableaux des proportions en fonction de la réalité.

```{r}
proptab2 <- round(prop.table(tab2, margin = 2), 3)
proptab2 %>% kable(caption = "Matrice des proportions en fonction de la réalité") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2))
```

```{r}
ERR_qda <- round(mean(realite != prediction),3)
ERR1_qda <- round(1-(tab2[2,2]/sum(tab2[,2])),3)
ERR2_qda <- round(1-(tab2[1,1]/sum(tab2[,1])),3) 
```

Nous remarquons que :

- Notre modèle a une erreur globale de `r ERR_qda*100`% ce qui est plus que pour la LDA et donc moins performant.

  - L'erreur de type I est de `r ERR1_qda*100`%, bien moins que celle en LDA.
  - Cependant, l'erreur de type II est de `r ERR2_qda*100`% ce qui correspond au taux de faux négatifs, c'est-à-dire l'erreur d'affectation d'articles populaires qui sont prédits impopulaires.

- Le taux de bonnes affectations est de `r round((tab2[1]+tab2[4])/length(dfbis2_test$popularity),3)*100`%.

  - Seulement `r proptab2[1]*100`% des articles populaires sont biens prédits.

  - `r proptab2[4]*100`% des articles impopulaires sont biens prédits.

Le modèle QDA est plus performant pour prédire si un article va être impopulaire alors qu'il se trompe plus pour prédire si un article sera populaire.

```{r, echo=FALSE, fig.height = 3.2, fig.width = 5.2}
popular<-sum(dfbis2_test$popularity=="Popular") 
unpopular<-sum(dfbis2_test$popularity=="Unpopular")
Error<-NULL 
ErrorI<-NULL
ErrorII<-NULL 
for(i in 1:101){
  c<-(i-1)/200 
  Prediction <- rep("Popular", nrow(dfbis2_train)) 
  Prediction[qda_res$posterior[,2]>c]<-"Unpopular"
  Error[i]<-sum(Prediction!=realite)/nrow(dfbis2_train) 
  ErrorI[i]<-sum((Prediction=="Popular")&(realite=="Unpopular"))/unpopular
  ErrorII[i]<-sum((Prediction=="Unpopular")&(realite=="Popular"))/popular
  } 

seuil <- seq(0, 0.5, by = (0.5/100))
df_error <- cbind(seuil, Error, ErrorI, ErrorII)
df_error <- as.data.frame(df_error)

p <- ggplot(data=df_error)  + geom_line(data=df_error, aes(x=seuil, y=ErrorI, colour = "1-Spécificité (Erreur type I)"))+ geom_line(data=df_error, aes(x=seuil, y=ErrorII, colour = "1-Sensibilité (Erreur type II)")) + geom_line(data=df_error, aes(x=seuil, y=Error, colour = "Erreur Globale")) + ggtitle("Graphiques des différentes erreurs de la QDA")+theme_minimal() + labs(color = "Erreurs")+ scale_color_manual(values=c("steelblue", "violetred", "slategray"))
p
```

Ce graphique nous montre que l'erreur de type II est beaucoup plus élevée que l'erreur globale ou que l'erreur de type I. Ces conclusions sont en accord avec celles faites au dessus comme quoi la QDA prédit mieux les articles impopulaires car ceux-ci sont représentés par l'erreur de type I.

Nous avons ensuite tracé la courbe ROC allant avec ce modèle.

```{r, fig.height = 3.2, fig.width = 5.2}
roc_qda <- roc(realite, qda_res$posterior[, 2])
ggroc(roc_qda) + ggtitle("Courbe ROC QDA") +
  geom_abline(intercept = 1, slope = 1, lty= "dashed") + theme_minimal()
```
```{r}
auc_qda <- round(roc_qda$auc[1],3)
```

La courbe ROC reste relativement proche de la bissetrice tout comme pour la LDA. De plus, l'aire sous la courbe est de `r auc_qda`.

## Comparaison LDA et QDA

Nous pouvons faire une rapide comparaison entre les deux modèles faits jusqu'ici, la LDA et la QDA. 

```{r, fig.height = 3.2, fig.width = 5.2}
ggroc(list(LDA=roc_lda, QDA=roc_qda)) +
  ggtitle("Récapitulatif des courbes ROC")+ labs(color="Modèle")+
  geom_abline(intercept = 1, slope = 1) + theme_minimal() +  scale_color_manual(values=c( "violetred","steelblue"))
```

Nous pouvons voir grâce au graphique que la différence entre les deux modèles est minime, même si la LDA semble légèrement plus performantes puisque la courbe est plus éloignée de la bissectrice.

<br>

\newpage

# K plus proches voisins

Pour effectuer notre modèle sur les K plus proches voisins, nous reprennons notre échantillon utilisé lors de la QDA (avec les 3 variables qui posent un problème de colinéarité en moins).
Nous testons en premier lieu un modèle des KNN avec un nombre de voisins de 6 (racine carrée de notre nombre de variables).

```{r, echo=FALSE}
load("knn.RData")
load("knn_tune.RData")
```

```{r, echo=FALSE}
realite <- dfbis2_test$popularity
prediction <- knn_pred
tabknn <- table(prediction, realite) %>% addmargins() %>% kable(caption = "Matrice de confusion") %>% kable_styling(full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2, ""))
tabknn
```

- Nous obtenons une erreur globale de `r round(mean(dfbis2_test[,55] != knn_pred),3)*100`% ce qui n'est pas très performant comparé à nos modèles précédents.

Afin d'améliorer ce modèle des K plus proches voisins, nous allons déterminer le nombre de voisins optimal minimisant l'erreur d'entrainement. Pour cela nous faisons une optimisation de nos KNN à l'aide d'une validation croisée.

```{r, echo=FALSE, fig.height = 3, fig.width = 5}
ggplot()+aes(x=knn_tune$performances[,1], y=knn_tune$performances[,2])+
  geom_line(color = "steelblue", size = 0.75)+theme_minimal()+
  geom_point()+
  labs(y = "Error", x="K", title="Erreurs d'entrainement des KNN")
```

```{r, echo=FALSE}
best_error <- round(knn_tune$best.performance*100, 3)
Tab<-cbind(knn_tune$best.parameters, best_error)
colnames(Tab)<-c("K","Erreur d'entrainement")
Tab %>% kable(caption = "Meilleur nombre de voisins") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover"))
```

```{r}
load("knn_best.RData")
```

Notre nombre optimal de voisins est  de 18. Nous allons donc refaire notre modèle en partant avec cette fois-ci 18 centres initiaux.
Nous obtenons cette matrice de confusion. 

```{r, echo=FALSE}
tab_knn <- table(knn.pred.prob2, dfbis2_test[,55]) %>% addmargins() %>% kable(caption = "Matrice de confusion") %>% kable_styling(full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2, ""))
tab_knn
```

```{r}
ERR_knn <- round(mean(dfbis2_test[,55] != knn.pred.prob2),3)
```

- Notre erreur globale est maintenant de `r ERR_knn*100`%. Comme prévu, ce modèle est plus performant que le précédent grâce à l'optimisation du nombre de centre. Même s'il reste moins efficace que la LDA ou la QDA.

```{r, echo = FALSE}
sol3 <- confusionMatrix(knn.pred.prob2, dfbis2_test[,55])
sol3$byClass %>% kable(caption = "Sommaire") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) 
```

```{r}
ERR1_knn <- round(1-(sol3$table[2,2]/(sol3$table[1,2]+sol3$table[2,2])),3)
ERR2_knn <- round(1-(sol3$table[1,1]/(sol3$table[1,1]+sol3$table[2,1])),3)
```

Grâce à ce résumé nous pouvons voir que la sensibilité et la spécificité ne sont pas très hautes ce qui n'est pas bon. Cela veut dire que les articles qu'ils soient populaires ou impopulaires sont mal identifiés dans notre base de données. Nous notons quand même qu'encore une fois la spécificité est plus importante, ce qui veut dire que les articles impopulaires sont légèrement mieux prédits. De plus, l'accuracy ici a une valeur très basse, notre modèle de KNN, même amélioré identifie et prédit très mal nos données.

<br>

\newpage

# Arbres de classification 
## Arbres de décisions

Dans cette partie, nous avons utilisé le même *dataframe* que pour la QDA.
Dans un premier temps nous avons fait un arbre de décisions avec toutes nos variables (`r ncol(dfbis)-1`), cependant celui-ci n'était pas lisible car notre base de données est trop volumineuse.

```{r, echo=FALSE}
control.max <- rpart.control(cp = 0, max.depth = 0, minbucket = 1, minsplit = 1)
load("tree.RData")
```

Mais nous avons quand même pu faire une matrice de confusion et une matrice de proportions en fonction de la réalité. 

```{r, echo=FALSE}
pred.tree <- predict(tree, newdata = dfbis_test, type = "class")
tabarbre <- table(pred.tree, dfbis_test[,47]) %>% addmargins() %>% kable(caption = "Matrice de confusion") %>% kable_styling(full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2, ""))
tabarbre
```

```{r, echo=FALSE}
proptabarbre <- round(prop.table(table( pred.tree, dfbis_test[,47]), margin = 1), 3)
proptabarbre %>%  kable(caption = "Proportion en fonction de la réalité") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2))
```

- Nous observons une erreur globale de `r round(mean(pred.tree != dfbis_test[,47]),3)*100`%.
- Nous avons `r proptabarbre[1]*100`% d'articles populaires et `r proptabarbre[4]*100`% d'articles impopulaires qui sont bien prédits.

Nous pouvons aussi afficher l'importance des variables.

```{r, echo=FALSE, fig.height = 4.2, fig.width = 6}
data.frame(vi = tree$variable.importance, variable = names(tree$variable.importance)) %>%
  ggplot() + aes(x= reorder(variable,vi), y = vi) + 
  geom_col(width = 0.4, color = "steelblue", fill = "steelblue") +
  coord_flip() + labs(y = "Mesure d'importance", x = NULL) + theme_minimal()
```
Ce graphique nous montre que les variables les plus importantes ici sont **n_tokens_content**, **n_unique_tokens** et **kw_avg_avg**. Ces variables sont en rapport avec les textes des articles. Nous pouvons donc supposer que ce modèle se base sur le contenu des articles pour pouvoir les affecter à une catégorie (*popular* ou *unpopular*).

Par la suite, pour pouvoir avoir un arbre lisible, nous avons décidé d'en construire un nouveau en changeant le paramètre de complexité. Pour cela, nous avons utilisé tidymodels pour pouvoir optimiser celui-ci.

```{r, echo=FALSE}
rec <- recipe(popularity ~ ., data = dfbis_train)
tree_spec <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

```{r, echo=FALSE}
tune_tree_wf <- workflow() %>% 
  add_model(tree_spec %>% 
              set_args(cost_complexity = tune())
            ) %>%
  add_recipe(rec)

df_cv <- vfold_cv(dfbis_train) 

cost_complexity_grid <- grid_regular(cost_complexity(range = c(-5,-0.1)), 
                                     levels = 15)
```

```{r, echo=FALSE}
load("tree_tune_res.RData")
```

```{r,  echo=FALSE,  echo=FALSE, fig.height = 3, fig.width = 4.8}
autoplot(tree_tune_res) + theme_minimal() +labs(title="Valeur du paramètre de complexité")
```
```{r, echo=FALSE}
best_cost_complexity <- select_best(tree_tune_res)
```

Ce graphique nous donne les valeurs de l'accuracy en fonction de différentes valeurs pour le paramètre de complexité. La valeur du paramètre de complexité qui maximise l'accuracy est de `r round(best_cost_complexity$cost_complexity,5)`.

```{r}
final_tune_tree_wf <- tune_tree_wf %>% 
  finalize_workflow(best_cost_complexity)
```

```{r}
tree_fit <- final_tune_tree_wf %>% last_fit(dfbis_split)
```

```{r}
final_tune_tree_wf <- tune_tree_wf %>% 
  finalize_workflow(best_cost_complexity)
```

Nous avons donc pu recréer un arbre cette fois-ci avec le paramètre de complexité optimal. Voici la version élagué de notre arbre.

```{r, fig.height = 4.8, fig.width = 7.7}
tree_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot::prp(type = 0, extra = 1, split.box.col = "steelblue",
                  roundint = FALSE)
```

Pour pouvoir mieux distinguer les variables qui sont importantes dans notre modèle il est possible de faire un diagramme en bâtons. Nous pouvons donc voir que cette fois-ci ce ne sont plus les mêmes variables qui interviennent dans le modèle. Effectivement, la variable **channel** à une importance capital dans la répartition entre populaire et impopulaire, puis viennent **kw_avg_avg** qui se rapportent au mots-clés dans l'article et enfin **LDA_04** qui à un lien avec les thèmes. 

```{r}
tree_fit %>% pluck(".workflow", 1) %>%  pull_workflow_fit() %>% vip(num_features = 20) + theme_minimal()
```

```{r}
load("final_model.RData")
```

Puis la matrice de confusion de ce modèle.

```{r}
mat <- augment(final_model, new_data=dfbis_test) %>% 
  conf_mat(truth=popularity, estimate=.pred_class)
matconf <- kable(mat$table,  caption = "Matrice de confusion") %>% kable_styling(full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2))
matconf
```

```{r}
ERR_tree <- round((mat$table[2]+mat$table[3])/sum(mat$table),3)
ERR1_tree <- round(1-(mat$table[1]/(mat$table[1]+mat$table[2])),3)
ERR2_tree <- round(1-(mat$table[4]/(mat$table[3]+mat$table[4])),3)
```

- Ce nouveau modèle a une erreur globale de `r ERR_tree*100`%.
- Nous avons également `r round((mat$table[1]/(mat$table[1]+mat$table[3])),3)*100`% d'articles populaires et `r round((mat$table[4]/(mat$table[2]+mat$table[4])),3)*100`% d'articles impopulaires qui sont bien prédits. Cette fois-ci, il y a peu de différence entre la prédictions des articles de nos deux classes.

Et pour terminer la courbe ROC du modèle.

```{r, echo=FALSE, fig.height=3.8, fig.width=5.8}
augment(final_model, new_data=dfbis_test) %>% 
roc_curve(as.factor(popularity),.pred_Popular) %>% 
autoplot()
```

Le tableau ci-dessous nous montre l'accuracy et l'AUC du modèle élagué, avec le meilleur paramètre de complexité.

```{r, echo=FALSE}
tree_fit <- final_tune_tree_wf %>% last_fit(dfbis_split)
tree_fit %>% collect_metrics() %>%  kable(caption = "Accuracy et AUC") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover"))
```

<br>

## Forêts aléatoires et bagging
### Nombre d'arbres

La base de données utilisée ici est la même que pour la LDA et les arbres de décisions.
Dans un premier temps, regardons ce que donne un modèle de forêt aléatoire et de bagging en terme d'erreur Out-Of-Bag. Nous cherchons donc le nombre d'arbres qui minimise cette erreur.

Pour le bagging, il s'agit d'un cas particulier de forêt aléatoire construit avec tous les prédicteurs (`r ncol(dfbis) - 1` variables).

```{r, echo=FALSE}
load("rf.RData")

oob_rf_min <- min(rf$err.rate[,1])
test_rf_min <- min(rf$err.rate[,2])
test2_rf_min <- min(rf$err.rate[,3])

best.ntrees <- which(abs(rf$err.rate[, 1] - oob_rf_min) < 0.0005) %>% min()

best.ntrees.test <- which(abs(rf$err.rate[, 2] - test_rf_min) < 0.0005) %>% min()

best.ntrees.test2 <- which(abs(rf$err.rate[, 3] - test2_rf_min) < 0.0005) %>% min()
```

```{r, echo=FALSE}
nvar <- ncol(dfbis) - 1
load("bag.RData")

oob_bag_min <- min(bag$err.rate[,1])
test_bag_min <- min(bag$err.rate[,2])
test_bag_min <- min(bag$err.rate[,3])

best.ntrees.bag <- which(abs(bag$err.rate[, 1] - oob_bag_min) < 0.0005) %>% min()

bag_err <- round(mean(predict(bag, newdata = dfbis_test,
                             type = "class") != dfbis_test[,47]),4)
```

```{r, echo=FALSE, fig.width=7}
ggplot()+
  geom_line(aes(1:1000, rf$err.rate[,1], color = "rf"))+ theme_minimal()+labs(y = ("Erreur"), x = ("Nombre d'arbres"), title = "Erreurs OOB selon le nombre d'arbres")+
  geom_line(aes(1:1000, bag$err.rate[,1], color = "bagging"))+ theme_minimal()+labs(y = ("Erreur"), x = ("Nombre d'arbres"))+
  geom_vline(xintercept = best.ntrees, linetype = "dashed")+
  geom_vline(xintercept = best.ntrees.bag, linetype = "dashed")+
  annotate("text", label = "756", x = best.ntrees + 50, y = 0.4)+
  annotate("text", label = "739", x = best.ntrees.bag - 50, y = 0.4)+
  scale_color_manual(values=c("steelblue", "violetred")) + labs(color="Modèles")
```

Nous remarquons que la courbe du modèle de forêt aléatoire est légèrement plus basse que celle du bagging.
Le nombre d'arbres optimal qui minimise l'erreur Out-Of-Bag pour le modèle de forêt aléatoire est de `r best.ntrees` arbres alors qu'on obtient `r best.ntrees.bag` arbres pour le bagging. Au-delà, l'erreur des deux modèles ne s'améliore pas car les courbes restent constantes. 

Cependant, nous obtenons sensiblement les mêmes erreurs autour de 450 arbres ce qui est moins coûteux en terme de ressources. 

Comparons ensuite ce que nous donne ces modèles avec 450 arbres contre les modèles avec le nombre d'arbres optimal. 

```{r}
load("best_bag.RData")

best_bag_err <- round(mean(predict(best_bag, newdata = dfbis_test,
                             type = "class") != dfbis_test[,47]),4)
```

```{r}
load("best_rf.RData")

test_best_rf <- round(mean(predict(rfq6, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47]),4)
```

```{r}
load("rf_min.RData")

test_rf_min <- round(mean(predict(rf_min, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47]),4)
```

```{r}
load("bag_min.RData")

bag_min_err <- round(mean(predict(bag_min, newdata = dfbis_test,
                             type = "class") != dfbis_test[,47]),4)
```

```{r}
vec_erreur_test <- c(bag_min_err, test_rf_min)
vec_nb_arbres <- c(best_bag_err, test_best_rf)
tab3 <- data.frame(vec_erreur_test)
tab3 <- cbind(tab3, vec_nb_arbres)
colnames(tab3) <- c("Erreur test (ntree optimal)", "Erreur test (ntree = 450)")
rownames(tab3) <- c("Bagging", "Random forest")
tab3 %>%  kable(caption = "Erreurs random forest vs bagging") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover"))
```
 
 
Le meilleur modèle de bagging est celui composé de `r best.ntrees.bag` arbres, il cumule sur nos données de test une erreur globale de `r bag_min_err*100`% mais reste très proche de l'erreur test avec 450 arbres qui est de `r best_bag_err*100`%.

Ces erreurs associées au bagging restent moins bonnes que les erreurs du modèle de forêt aléatoire qui sont de :

 - `r test_rf_min*100`% d'erreur pour `r best.ntrees` arbres 
 - `r test_best_rf*100`% pour 450 arbres.

### Nombre de variables

Nous allons maintenant regarder quel serait le meilleur nombre de variable à selectionner pour une forêt aléatoire de 450 arbres.

```{r}
load("rfq1.RData")
load("rfq4.RData")
load("rfq6.RData")
load("rfq8.RData")
load("rfq16.RData")
load("rfq32.RData")
```

```{r, fig.width=7}
ggplot() + geom_line(aes(1:450, rfq1$err.rate[, 1], color="q = 1")) + 
  geom_line(aes(1:450, rfq4$err.rate[, 1], color="q = 4")) +
  geom_line(aes(1:450, rfq6$err.rate[, 1], color="q = 6"))+
  geom_line(aes(1:450, rfq8$err.rate[, 1], color="q = 8")) +
  geom_line(aes(1:450, rfq16$err.rate[, 1], color="q = 16")) +
  geom_line(aes(1:450, rfq32$err.rate[, 1], color="q = 32"))+
  labs(color = "Nombre de variables", title="Erreur OOB du Random Forest \n selon le nombre de variables", y = "Erreur OOB", x = "Nombre d'arbres")+theme_minimal()+  scale_color_manual(values=c("orange", "steelblue", "darkseagreen","black","lightsalmon4","violetred"))
```
Ces différences d'erreurs bien que très minimes montrent que le modèle avec 4 variables est le plus otpimal pour 450 arbres. 
C'est d'autant plus le cas que notre échantillon d'entrainement contient relativement beaucoup d'articles (`r nrow(dfbis_train)` individus). Le tableau récapitulatif suivant confirme les conclusions tirées du graphique.

```{r}
test_rfq1 <- mean(predict(rfq1, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47])
test_rfq4 <- mean(predict(rfq4, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47])
test_rfq6 <- mean(predict(rfq6, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47])
test_rfq8 <- mean(predict(rfq8, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47])
test_rfq16 <- mean(predict(rfq16, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47])
test_rfq32 <- mean(predict(rfq32, newdata = dfbis_test, type = "class")
                          != dfbis_test[,47])

oob_rfq1_min <- min(rfq1$err.rate[,1])
oob_rfq4_min <- min(rfq4$err.rate[,1])
oob_rfq6_min <- min(rfq6$err.rate[,1])
oob_rfq8_min <- min(rfq8$err.rate[,1])
oob_rfq16_min <- min(rfq16$err.rate[,1])
oob_rfq32_min <- min(rfq32$err.rate[,1])
```

```{r}
vec_erreur_test_q <- round(c(test_rfq1, test_rfq4, test_rfq6, test_rfq8, test_rfq16, test_rfq32),4)
vec_erreur_OOB_q <- round(c(oob_rfq1_min, oob_rfq4_min,oob_rfq6_min, oob_rfq8_min, oob_rfq16_min, oob_rfq32_min),4)
tab_3 <- data.frame(vec_erreur_test_q)
tab_3 <- cbind(tab_3, vec_erreur_OOB_q)
colnames(tab_3) <- c("Err_test", "Err_OOB")
rownames(tab_3) <- c("RF : q=1", "RF : q=4", "RF : q=6", "RF : q=8", "RF : q=16", "RF : q=32")
tab_3 %>%  kable(caption = "Erreurs selon le nombre de variables") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Type d'erreur"=2))
```
Les erreurs sur nos données test étant très semblables, nous gardons le modèle de forêt aléatoire avec 4 variables (q = 4) car il présente la plus faible erreur Out-Of-Bag qui est de `r round(oob_rfq4_min,3)`.

En ce qui concerne la profondeur de nos arbres, les résultats en terme d'erreur n'étaient pas tellement impactés donc nous avons conservé le paramètre par défaut.

```{r, fig.height=3.8}
ggplot() + geom_line(aes(1:450, rfq4$err.rate[, 1], color="OOB")) + 
  geom_line(aes(1:450, rfq4$err.rate[, 2], color="Popular")) +
  geom_line(aes(1:450, rfq4$err.rate[, 3], color="Unpopular"))+
  labs(color = "Modalités", title="Erreurs du meilleur modèle", y = "Erreur", x = "Nombre d'arbres")+theme_minimal()+  scale_color_manual(values=c("violetred", "steelblue", "darkseagreen"))
```
Nous remarquons que l'erreur du groupe *Unpopular* est la plus basse, comme pour quasiment tous nos modèles précédents. On en conclut qu'avec notre meilleur modèle de forêt aléatoire, les articles impopulaires sont un peu mieux prédits que les articles populaires. 
Au final, notre meilleur modèle nous donne une erreur globale de `r round(test_rfq4,3)`

```{r}
ERR_rf <- round(test_rfq4,3)

ERR1_rf <- round(mean(predict(rfq4, newdata = dfbis_test[dfbis_test$popularity == "Popular",], type = "class")
     != dfbis_test[dfbis_test$popularity == "Popular",47]),3)

ERR2_rf <- round(mean(predict(rfq4, newdata = dfbis_test[dfbis_test$popularity == "Unpopular",], type = "class")
     != dfbis_test[dfbis_test$popularity == "Unpopular",47]),3)
```

### Importance des variables

```{r, fig.height=4.6, fig.width=7}
varImpPlot(rfq4, cex = 0.75, main = "Importance des variables du meilleur \n modèle de forêt aléatoire (q = 4)")
```

Nous retrouvons avec ce graphique les variables qui jouent un rôle important dans notre meilleur modèle de forêt aléatoire avec 4 variables. On retrouve parmi ces variables **kw_avg_avg** et **kw_max_avg** qui sont celles qui influencent le plus notre modèle et qui se rapportent au mots-clés pouvant se trouver dans les articles. Après, il y a **self_reference_min_shares** ou encore les variables **LDA_02**, **LDA_04** et **LDA_00** qui représenteraient des thèmes des articles.

## Boosting 

Dans ces derniers modèles la base de données utilisées est la même que pour la LDA. Le but de cette partie va être de comparer différents modèles de boosting. Le premier se fera sur arbre complet, le deuxième sera un boosting avec stumps et enfin deux modèles de boosting avec une pénalisation de 0.01 et de 0.001.

```{r, echo = FALSE}
load("boost.RData")
load("booststump.RData")
load("boostpen01.RData")
load("boostpen001.RData")
load("best_boost.RData")
```

```{r, fig.width=6.6, fig.height=3.5}
niter <- 200 
data.frame(iter = 1:niter) %>% 
  mutate(boost = boost$model$errs[1:niter, c("train.err")],
         stump = booststump$model$errs[1:niter, c("train.err")],
         pen01 = boostpen01$model$errs[1:niter, c("train.err")],
         pen001 = boostpen001$model$errs[1:niter, c("train.err")]) %>%
  pivot_longer(cols = 2:5, names_to = "model", values_to = "error") %>%
  ggplot() + aes(x = iter, y = error, color = model) + geom_line() + labs(color = "Modèles")+ 
  labs(title = "Differents modèles de boosting",subtitle =" et leurs erreurs en apprentissage", x = "Iterations", y = "Erreurs") + theme_minimal()+  scale_color_manual(values=c("steelblue", "violetred", "slategrey","black"))
```
Nous constatons que l'erreur sur les données d'entrainement du boosting chute très rapidement à zéro et que le modèle stump se stabilise en terme d'erreur à partir d'environ 50 itérations. Les modèles de boosting avec pénalisation semblent avoir une erreur constante selon le nombre d'itération.

```{r, fig.width=6.8, fig.height=3.7}
niter <- 200
1:niter %>% as_tibble() %>% 
  rename("iter" = value) %>% 
  mutate(boost_test = boost$model$errs[1:niter, 3],
         booststump_test = booststump$model$errs[1:niter, 3],
         boostpen01_test = boostpen01$model$errs[1:niter, 3],
         boostpen001_test = boostpen001$model$errs[1:niter, 3]) %>%
  pivot_longer(cols = 2:5, names_to = "model", values_to = "error") %>%
  ggplot() + aes(x = iter, y = error, color = model) + geom_line() +
  labs(title = "Differents modèles de boosting",subtitle =" et leurs erreurs en test", x = "Iterations", y = "Errors") +theme_minimal()+labs(color="Modèles")+scale_color_manual(values=c("steelblue", "violetred", "slategrey","black"))
```
Grâce à ce grpahique nous pouvons tout de suite voir que le modèle avec les erreurs en test les plus basses est le modèle avec les stumps.
Nous remarquons aussi que l'erreur test du boosting semble continuer de descendre. 
Nous avons donc regardé cette erreur sur 500 d'itérations. Nous n'avons pas fait 500 iterations dès le début car cela est très couteux pour l'ordinateur de plus cela n'aurait pas servi pour les modèles avec pénalisation et stumps car ils semblent se stabiliser avant les 200 iterations.

```{r, echo=FALSE, fig.width=6.8, fig.height=3.7}
niter <- 500
1:niter %>% as_tibble() %>% 
  rename("iter" = value) %>% 
  mutate( best_boost_test = best_boost$model$errs[1:niter, 3]) %>%
  pivot_longer(cols = 2, names_to = "model", values_to = "error") %>%
  ggplot() + aes(x = iter, y = error, color = model) + geom_line() +
  labs(title = "Boosting", x = "Iterations", y = "Errors") +theme_minimal()+labs(color="Modèle")+scale_color_manual(values=c("steelblue"))
```
Ce graphique nous permet de voir que le boosting était le plus performant à 350 itérations environ, au delà l'erreur remontait. Nous pouvons supposer la présence d'un phénomène de surapprentissage.

```{r, echo=FALSE}
nbr_iter<-c(50,100,200)
tab_boostpen01<-c()
tab_boostpen001<-c()
tab_stump<-c()
for (i in 1:3){
    tab_boostpen01[i]<-round(mean(boostpen01$model$errs[nbr_iter[i],"test.err"]),3)
    tab_boostpen001[i]<-round(mean(boostpen001$model$errs[nbr_iter[i],"test.err"]),3)
    tab_stump[i]<-round(mean(booststump$model$errs[nbr_iter[i],"test.err"]),3)
}
for (i in 4:5){
    tab_boostpen01[i]<- " "
    tab_boostpen001[i]<- " "
    tab_stump[i]<-" "
}
```

```{r}
nbr_iter<-c(50,100,200,350,400)
tab_bestboost <- c()
for (i in 1:5){
    tab_bestboost[i]<-round(mean(best_boost$model$errs[nbr_iter[i],"test.err"]),3)
}
```

```{r, echo=FALSE}
meilleure_erreur<-rbind(tab_bestboost, tab_boostpen01,tab_boostpen001,tab_stump)
colnames(meilleure_erreur)<-c(nbr_iter)
row.names(meilleure_erreur)<-c("Boosting","Boost Pen 10%", "Boost Pen 1%", "Stump")
meilleure_erreur %>% kable(caption = "Les erreurs en test des modèles") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) 
```
Ce tableau confirme les observations faites auparavant grâce au graphique. Le meilleur modèle est celui avec les stumps et 200 itérations. Ceci était également visible grâce aux graphiques puisque sur le premier graphique avec les erreurs test nous pouvons voir que les erreurs pour le modèle stumps descendent en dessous de 0.35. Or, sur le graphique avec uniquement les erreurs du boosting la courbe ne va pas en dessous de 0.35.

Nous avons fait la matrice de confusion associée au modèle stumps.

```{r}
realite <- dfbis_test$popularity
prediction <- predict(booststump, newdata = dfbis_test)
tabstump <- table(prediction, realite)
addmargins(tabstump) %>% kable(caption = "Matrice de confusion") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) %>% add_header_above(c("","Réalité"=2, ""))
```

```{r}
ERR_stump <- round((tabstump[1,2] + tabstump[2,1]) / sum(tabstump),3)
ERR1_stump <- round(1-(tabstump[2,2]/sum(tabstump[,2])),3)
ERR2_stump <- round(1-(tabstump[1,1]/sum(tabstump[,1])),3)
```

- L'erreur globale de ce modèle est de `r ERR_stump`.
- L'erreur de type I est de `r ERR1_stump`, il y a donc `r ERR1_stump*100`% d'articles qui sont prédits comme étant populaires alors qu'en fait ils sont impopulaires.
- L'erreur de type II est de `r ERR2_stump`, il y a donc `r ERR2_stump*100`% d'articles qui sont prédits comme étant impopulaires alors qu'en fait ils sont populaires.
- L'accuracy est de `r round(1-ERR_stump,3)`.

<br>

\newpage

# Comparaison des modèles et conclusion 

Pour faire la comparaison entre nos modèles nous allons nous baser sur les erreurs globales et donc sur l'accuracy. Pour se faire nous allons choisir nos meilleurs modèles pour chaque partie, c'est-à-dire : la LDA, la QDA, les KNN avec 18 voisins, l'arbre élagué, la forêt aléatoire avec 4 variables et enfin le boosting avec stumps et 200 itérations.

```{r}
final_tab <- matrix(c(1-ERR_lda, ERR_lda, ERR1_lda, ERR2_lda, 1-ERR_qda, ERR_qda, ERR1_qda, ERR2_qda, 1-ERR_knn, ERR_knn, ERR1_knn, ERR2_knn, 1-ERR_tree, ERR_tree, ERR1_tree, ERR2_tree, 1-ERR_rf, ERR_rf, ERR1_rf, ERR2_rf, 1-ERR_stump, ERR_stump, ERR1_stump, ERR2_stump), byrow = TRUE, ncol = 4)
rownames(final_tab) <- c("LDA", "QDA", "KNN", "Tree", "Random forest", "Booststump")
colnames(final_tab) <- c("Accuracy", "Erreur globale", "Faux populaires", "Faux impopulaires")

final_tab %>% kable(caption = "Conclusion") %>% 
  kable_styling(
  full_width = FALSE, position = "center",bootstrap_options = c("striped", "hover")) 
```

Nous pouvons conclure que notre meilleur modèle est le modèle de forêt aléatoire avec 4 variables et 450 arbres car il a la plus basse erreur sur les données test et donc la meilleure performance (`r (1-ERR_rf)*100`% d'accuray). 
Cependant, dans une optique de prédire si un article va être impopulaire, le modèle QDA s'avère être le plus performant du fait de son erreur de type I la plus faible.
En revanche, l'enjeu de prédire la popularité des articles va plutôt être de prédire si un article donné va être populaire. Ainsi, notre meilleur modèle de forêt aléatoire est le plus adapté du fait de son erreur de type II (`r ERR2_rf`) et de test (`r ERR_rf`) qui sont les plus petites en comparaison avec tous les autres modèles.

Nous avons aussi pu remarquer que les articles impopulaires étaient en général mieux identifés et donc mieux prédits dans la quasi-totalité de nos modèles. Cela peut s'expliquer par la répartiton difforme du groupe *Popular* par rapport au groupe *Unpopular*. Pour rappel, ces classes sont séparées à la médiane du nombre de partages qui est de `r median(df$shares)` partages, sauf que le nombre de partages maximal est de `r max(df$shares)` partages. La classe des articles populaires est donc très étendue par rapport à celle des articles impopulaires.

Nos modèles pourraient être améliorés en effectuant une ACM au préalable sur nos données afin de réduire le bruit et donc espérer une meilleure performance de prédiction. Choisir une plus grande proportion de données d'entrainement peut aussi être une solution, nous pourrons proposer 3/4. Ou encore effectuer ces prédictions sur 3 modalités au lieu de 2 pour avoir une classe dédiée aux valeurs extrêmes que représentent les articles les plus partagés. 





